{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7908702f-e93a-42fe-8c72-ea98bf3c127a",
   "metadata": {},
   "source": [
    "Copyright Ann Arbor Algorithms Inc. 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5526b193-5335-448b-a68d-e98b582543d9",
   "metadata": {},
   "source": [
    "Download pre-built binary: https://github.com/ggerganov/llama.cpp/releases \n",
    "\n",
    "See `build.sh` for build instruction.\n",
    "\n",
    "The generated binary program is called \"main\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43b29be-61d6-411d-b963-4c3e6b655494",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log start\n",
      "main: build = 3501 (b7a08fd5)\n",
      "main: built with cc (Ubuntu 13.2.0-23ubuntu4) 13.2.0 for x86_64-linux-gnu\n",
      "main: seed  = 1722544722\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from ../models/gguf/ggml-model-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3.1-8B-bnb-4bit\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = smaug-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128004\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3.1-8B-bnb-4bit\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: PAD token        = 128004 '<|finetune_right_pad_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 131072\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size = 16384.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 16384.00 MiB, K (f16): 8192.00 MiB, V (f16): 8192.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  8480.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "\n",
      "system_info: n_threads = 8 / 32 | AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "sampling: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampling order: \n",
      "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temperature \n",
      "generate: n_ctx = 131072, n_batch = 2048, n_predict = 128, n_keep = 0\n",
      "\n",
      "\n",
      "I believe the meaning of life is to give life to others. When we give life to others, we receive life in return. So, I believe that the meaning of life is to make a positive difference in the world and to help others grow and develop.\n",
      "I have always been drawn to the idea of being a mentor and helping others to reach their full potential. I believe that every person has a unique gift or talent to share with the world, and it is our responsibility to help them discover and develop that gift.\n",
      "For me, the idea of being a mentor is not just about giving advice or guidance, but about being a role model and showing others what it means to\n",
      "llama_print_timings:        load time =    6591.37 ms\n",
      "llama_print_timings:      sample time =       6.80 ms /   128 runs   (    0.05 ms per token, 18817.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     181.78 ms /     7 tokens (   25.97 ms per token,    38.51 tokens per second)\n",
      "llama_print_timings:        eval time =   11914.67 ms /   127 runs   (   93.82 ms per token,    10.66 tokens per second)\n",
      "llama_print_timings:       total time =   12123.45 ms /   134 tokens\n",
      "Log end\n"
     ]
    }
   ],
   "source": [
    "! ./llama-cli -m ../models/gguf/ggml-model-Q4_K_M.gguf -p \"I believe the meaning of life is\" -n 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "144aaab7-595d-41ad-954f-cdf5b173ec87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is to be happy and fulfilled.  I think that's a pretty universal sentiment, too.  \n",
      "\n",
      "But what exactly does happiness and fulfillment mean?  And how can we achieve it? \n",
      "\n",
      "It's a question philosophers have grappled with for millennia!  There isn't a single, definitive answer, but here are some thoughts:\n",
      "\n",
      "**Defining Happiness and Fulfillment:**\n",
      "\n",
      "* **Happiness:**  This often feels like a fleeting, emotional high. It's the positive feelings we get from achievements, experiences, and even simple moments of joy. \n",
      "* **Fulfillment:** This feels deeper, more enduring. It"
     ]
    }
   ],
   "source": [
    " ! ./llama-cli-gpu -ngl 19 -m models/2b_it_v2.gguf -p \"I believe the meaning of life is\" -n 128 2> /dev/null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5bcf9d-7ddb-4d1c-8ca2-712c546e9dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is to find your purpose, and then use that purpose to make a positive impact on the world. – Unknown\n",
      "As humans, we are constantly searching for the meaning of life. What is our purpose? What should we be doing with our time? Should we be working, exploring, creating, or simply existing? These are the questions that have puzzled philosophers and thinkers for centuries.\n",
      "\n",
      "In my opinion, the meaning of life is not a fixed or definitive answer. Instead, it is a personal and dynamic concept that evolves as we grow and learn. For some, their purpose may be to contribute to their community, to help others, or to make"
     ]
    }
   ],
   "source": [
    "! ./llama-cli-gpu -ngl 33 -m models/Meta-Llama-3-8B-Instruct.Q4_0.gguf -p \"I believe the meaning of life is\" -n 128 2> /dev/null"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
